<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture 5: Algorithms and Algorithmic Bias - Summary</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            padding: 20px;
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            padding: 30px;
        }
        
        header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 1px solid #eee;
        }
        
        h1 {
            font-size: 28px;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        
        .subtitle {
            font-size: 18px;
            color: #7f8c8d;
        }
        
        h2 {
            font-size: 22px;
            color: #2c3e50;
            margin-top: 25px;
            margin-bottom: 15px;
            padding-bottom: 8px;
            border-bottom: 1px solid #eee;
        }
        
        h3 {
            font-size: 18px;
            color: #34495e;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        
        p {
            margin-bottom: 15px;
        }
        
        ul, ol {
            margin-bottom: 15px;
            padding-left: 25px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .summary-content {
            margin-bottom: 30px;
        }
        
        .nav-buttons {
            display: flex;
            justify-content: space-between;
            margin-top: 30px;
        }
        
        .nav-button {
            display: inline-block;
            padding: 10px 20px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            transition: background-color 0.3s;
        }
        
        .nav-button:hover {
            background-color: #2980b9;
        }
        
        footer {
            text-align: center;
            margin-top: 30px;
            color: #7f8c8d;
            font-size: 14px;
        }
        
        @media (max-width: 600px) {
            .container {
                padding: 20px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Lecture 5: Algorithms and Algorithmic Bias</h1>
            <p class="subtitle">One-Page Summary</p>
        </header>
        
        <section class="summary-content">
            <h2>Understanding Algorithms</h2>
            <p>AI algorithms provide instructions for AI technology to think and react to data in ways that are intuitive to human information processing. Machine learning algorithms are sets of rules or processes used by AI systems to conduct tasksâ€”most often to discover new data insights and patterns, or to predict output values from a given set of input variables. These algorithms enable machine learning (ML) to learn and adapt.</p>
            
            <h2>Algorithmic Bias</h2>
            <p>Algorithmic bias is a systematic deviation from equality that emerges in the outputs of an algorithm (Kordzadeh & Ghasemaghei, 2022). It can be viewed as discriminatory algorithmic outcomes that may adversely impact protected or unprotected groups due to inaccurate modeling that misses associations between output variables and input features (Akter et al., 2021).</p>
            
            <h3>Sources of Algorithmic Bias</h3>
            <ul>
                <li><strong>Biased Data:</strong> Training datasets that are not representative of the real world</li>
                <li><strong>Biased People:</strong> Products built for the benefit of one group while inadvertently producing side-effects for others</li>
                <li><strong>Biased Algorithms:</strong> The design and implementation of the algorithm itself</li>
            </ul>
            
            <h2>Types of Bias in Data-Driven Innovation</h2>
            
            <h3>Training Data Bias</h3>
            <ul>
                <li><strong>Selection Bias:</strong> Dataset examples chosen in a way that is not reflective of real-world distribution</li>
                <li><strong>Coverage Bias:</strong> Data not selected in a representative fashion</li>
                <li><strong>Non-response Bias:</strong> Unrepresentative data due to participation gaps in data collection</li>
                <li><strong>Sampling Bias:</strong> Improper randomization during data collection</li>
                <li><strong>Out-group Homogeneity Bias:</strong> Tendency to stereotype members of groups to which you don't belong</li>
            </ul>
            
            <h3>Method Bias</h3>
            <ul>
                <li><strong>Overgeneralization:</strong> Providing generic insights not suitable for specific contexts</li>
                <li><strong>Correlation Fallacy:</strong> Confusing correlation with causation</li>
                <li><strong>Confirmation Bias:</strong> Processing data in ways that affirm pre-existing beliefs</li>
                <li><strong>Automation Bias:</strong> Favoring results from automated systems regardless of error rates</li>
            </ul>
            
            <h3>Societal Bias</h3>
            <ul>
                <li><strong>Historical Bias:</strong> Historical data reflecting past inequities</li>
                <li><strong>Implicit Bias:</strong> Assumptions based on personal experiences that don't apply generally</li>
                <li><strong>Reporting Bias:</strong> Frequency of events in datasets not accurately reflecting real-world frequency</li>
            </ul>
            
            <h2>Algorithmic Bias in Practice</h2>
            
            <h3>Social Media and Recommendation Systems</h3>
            <ul>
                <li><strong>Filter Bubbles:</strong> News that users dislike or disagree with is automatically filtered out, narrowing what they know</li>
                <li><strong>Echo Chambers:</strong> Environments where users encounter only opinions similar to their own</li>
                <li><strong>Algorithmic Audiencing:</strong> Algorithms determine who sees what content, reshaping free speech</li>
            </ul>
            
            <h3>Collaborative Filtering Bias</h3>
            <ul>
                <li><strong>Cold-start Problem:</strong> New items with no ratings struggle to get recommended</li>
                <li><strong>Popularity Bias:</strong> Popular items are over-recommended</li>
                <li><strong>Over-specialization:</strong> Recommendations become narrower than the full range of user interests</li>
                <li><strong>Homogenization:</strong> Over time, recommendations for everyone start to look very similar</li>
            </ul>
            
            <h2>Case Study: Robo-debt (Australia)</h2>
            <p>The Robo-debt scandal in Australia involved an automated system for identifying welfare overpayments that resulted in many false debt notices.</p>
            <ul>
                <li><strong>Data Bias:</strong> The algorithm relied on past taxation records and assumed the same pattern of wages would continue</li>
                <li><strong>Method Bias:</strong> Used averages instead of actual earnings, leading to inflated or false debt amounts</li>
                <li><strong>Societal Bias:</strong> Targeted marginalized people based on income level and shifted the burden of proof onto welfare recipients with limited resources</li>
            </ul>
            
            <h2>Mitigating Algorithmic Bias</h2>
            <ul>
                <li>Ensure diverse and representative training data</li>
                <li>Implement fairness metrics and regular bias audits</li>
                <li>Include diverse perspectives in algorithm development</li>
                <li>Maintain human oversight of algorithmic decisions</li>
                <li>Establish clear accountability frameworks</li>
                <li>Provide transparency about how algorithms work</li>
            </ul>
        </section>
        
        <div class="nav-buttons">
            <a href="summaries.html" class="nav-button">Back to Summaries</a>
            <a href="ethics-flashcards.html" class="nav-button">Practice Flashcards</a>
        </div>
        
        <footer>
            <p>Created for exam preparation - April 2024</p>
        </footer>
    </div>
</body>
</html>
